# PC Voice Assist 架构文档

## 系统架构

PC Voice Assist 采用模块化设计,主要包含以下几个核心组件:

### 1. 语音交互层

- **语音识别模块** (`speech_recognition_module.py`): 使用 Google Speech Recognition API 将用户的语音输入转换为文本
- **语音合成模块** (`text_to_speech.py`): 使用 pyttsx3 将系统响应转换为语音输出

### 2. 智能决策层

- **LLM客户端** (`llm_client.py`): 
  - 与 OpenAI API 交互
  - 实现 Function Calling 机制
  - 管理对话上下文和历史
  - 支持多种大模型 (gpt-4.1-mini, gemini-2.5-flash等)

### 3. 任务执行层

- **任务执行引擎** (`task_executor.py`):
  - 解析大模型返回的函数调用
  - 路由到对应的控制器
  - 返回执行结果给大模型

### 4. 功能控制层

包含多个专门的控制器,每个控制器负责特定类型的操作:

- **音乐控制器** (`music_controller.py`): 播放、暂停、继续、停止音乐,搜索音乐文件,管理播放状态
- **写作控制器** (`writing_controller.py`): 使用大模型生成文章,支持不同长度的文章,自动保存到指定位置
- **文件控制器** (`file_controller.py`): 创建、读取、删除文件,路径安全验证,仅允许在安全目录内操作
- **应用控制器** (`app_controller.py`): 打开预定义的应用程序,跨平台支持,应用白名单机制
- **系统控制器** (`system_controller.py`): 音量控制,截图功能,系统级操作

## 数据流

```
用户语音 
  → 语音识别 
  → 文本
  → LLM理解意图
  → Function Calling
  → 任务执行引擎
  → 对应控制器
  → 执行结果
  → LLM生成回复
  → 语音合成
  → 用户听到回复
```

## Function Calling 机制

系统使用 OpenAI 的 Function Calling 功能来实现智能任务编排。首先在 `llm_client.py` 中定义所有可用的函数及其参数,然后 LLM 根据用户输入决定调用哪些函数。任务执行引擎调用对应的控制器执行函数,将执行结果返回给 LLM,最后 LLM 基于执行结果生成自然语言回复。

这种机制使得系统能够理解复杂的用户意图,自动组合多个功能,处理多步骤任务,并提供智能的错误处理和反馈。

## 安全机制

系统实现了多层安全保护。文件操作方面,只允许在配置的安全目录内操作,并通过路径验证防止目录遍历攻击。应用控制方面,采用应用白名单机制,只能打开预定义的应用程序。系统操作方面,限制可执行的系统操作类型,并进行参数范围验证。

## 扩展性设计

要添加新的控制器,需要在 `src/controllers/` 目录创建新的控制器类,继承基本的控制器模式,在 `task_executor.py` 中添加路由逻辑,并在 `llm_client.py` 中添加函数定义。要添加新的功能,需要在对应的控制器中添加新方法,在 Function Calling 定义中添加新的参数或函数,并更新任务执行引擎的路由逻辑。

## 配置管理

所有配置集中在 `config.py` 中,包括大模型配置、语音识别/合成配置、安全配置、路径配置和应用白名单。

## 错误处理

每个模块都实现了完善的错误处理机制,能够捕获异常并返回友好的错误信息,记录错误日志,并向用户提供清晰的反馈。

## 性能优化

系统实现了多项性能优化措施,包括对话历史限制以避免上下文过长,文件读取内容截断以避免返回过大内容,以及为未来扩展预留的异步执行支持。

